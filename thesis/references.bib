@misc{placeholder,
  title    = {Estimation of building height using a single street view image via deep neural networks},
  journal  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume   = {192},
  pages    = {83-98},
  year     = {2022},
  issn     = {0924-2716},
  doi      = {https://doi.org/10.1016/j.isprsjprs.2022.08.006},
  url      = {https://www.sciencedirect.com/science/article/pii/S0924271622002106},
  author   = {Yizhen Yan and Bo Huang},
  keywords = {Street view image, Building height, Single view, Uncertainty analysis, Deep learning},
  abstract = {Building smart cities requires three-dimensional (3D) modelling to facilitate the planning and management of built environments. This requirement leads to high demand for data on vertical dimensions, such as building height, that are critical for the construction of 3D city models. Despite increasing recognition of the importance of such data, their acquisition in a low-cost and efficient manner remains a daunting task. Big data, particularly street view images (SVIs), provide an opportunity to efficiently solve this problem. In this study, we aim to derive information on building height from openly available SVIs by using single view metrology. Unlike other methods using multisource inputs, our method capitalizes on deep neural networks to extract a set of features – such as vanishing points, line segments, and semantic segmentation maps – for single view measurement and then estimates the height from single SVIs. The minimal input required by the method increases its competitiveness in large-scale estimation of building heights, especially in areas with difficulty to obtain the conventional remote sensing data. In addition to experiments that demonstrate the effectiveness and efficiency of the proposed method, we also conduct a thorough analysis of uncertainties and errors brought by the method, thereby providing guidance for its future applications.1Source code: https://github.com/yzre/SIHE.}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{https://builtin.com/machine-learning/siamese-network,
  title        = {A Friendly Introduction to Siamese Networks},
  howpublished = {\url{https://builtin.com/machine-learning/siamese-network}},
  author       = {Sean Benhur},
  year         = {2022}
}
@misc{huang2023survey,
      title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}, 
      author={Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
      year={2023},
      eprint={2311.05232},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{Wolf2019HuggingFacesTS,
  title   = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author  = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1910.03771},
  url     = {https://api.semanticscholar.org/CorpusID:208117506}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sutskever2014sequence,
      title={Sequence to Sequence Learning with Neural Networks}, 
      author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
      year={2014},
      eprint={1409.3215},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yenduri2023generative,
      title={Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions}, 
      author={Gokul Yenduri and Ramalingam M and Chemmalar Selvi G and Supriya Y and Gautam Srivastava and Praveen Kumar Reddy Maddikunta and Deepti Raj G and Rutvij H Jhaveri and Prabadevi B and Weizheng Wang and Athanasios V. Vasilakos and Thippa Reddy Gadekallu},
      year={2023},
      eprint={2305.10435},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{raffel2023exploring,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{reimers2019sentencebert,
  title         = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author        = {Nils Reimers and Iryna Gurevych},
  year          = {2019},
  eprint        = {1908.10084},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@unknown{BERT,
  author = {Zanetti, Andrea},
  year   = {2021},
  month  = {06},
  pages  = {},
  title  = {Quantization Aware Training, ERNIE and Kurtosis Regularizer: a short empirical study}
}
@misc{lewis2021retrievalaugmented,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{llm-trends,
title={Large Language Models 101: History, Evolution and Future},
year={2022},
howpublished={\url{https://www.scribbledata.io/blog/large-language-models-history-evolutions-and-future/}}
}

@misc{illustrated-bert,
title={The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)},
year={2021},
howpublished={\url{http://jalammar.github.io/illustrated-bert/}},
author={Jay Alammar}
}

@misc{balestriero2023cookbook,
      title={A Cookbook of Self-Supervised Learning}, 
      author={Randall Balestriero and Mark Ibrahim and Vlad Sobal and Ari Morcos and Shashank Shekhar and Tom Goldstein and Florian Bordes and Adrien Bardes and Gregoire Mialon and Yuandong Tian and Avi Schwarzschild and Andrew Gordon Wilson and Jonas Geiping and Quentin Garrido and Pierre Fernandez and Amir Bar and Hamed Pirsiavash and Yann LeCun and Micah Goldblum},
      year={2023},
      eprint={2304.12210},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{
  mlm,
  title={How Does BERT NLP Optimization Model Work?},
  author={Aswini R},
  howpublished = {\url{https://www.turing.com/kb/how-bert-nlp-optimization-model-works/}},
}
